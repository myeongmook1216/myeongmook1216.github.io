# SIFT

## 발상(5.1)

- 버스를 추적하는 효과적인 방법 중 하나는 두 영상에서 feature point를 추출하고 매칭을 통해 해당하는 특징점 쌍을 찾는 것이다. (즉, corresspond problem을 풀어야 한다.)
- 반복성이 좋다
    - 왼쪽 영상에서 추출된 feature point가 오른쪽 영상에서도 추출되는 것.
- 불변성이 좋다.
    - 물체에 이동, 회전, 스케일 변환이 발생하더라도 특징점에서 추출한 두 벡터는 비슷해야 하는 것.
- 지역 특징
    - 좁은 지역을 보고 특징점 여부 판정
    - 반복성이 중요
        - 종류 : 위치, 스케일, 방향, 특징 기술자
        - 위치, 스케일 : detection 단계
        - 방향, 특징 기술자(feature descriptor) : description 단계

**지역 특징의 조건**

- repeatability : 같은 물체가 서로 다른 두 영상에 나타났을 때, 첫 번째 영상에서 검출된 feature point가 두 번째 영상에서도 같은 위치에서 높은 확률로 검출되어야 한다.
- invariance : 물체에 이동, 회전, 스케일 변환이 발생해도 feature descriptor의 값은 비슷해야 한다.
- discriminative power : 물체의 다른 곳에서 추출된 특징과 두드러지게 달라야 한다.
    - 그렇지 않으면, 물체의 다른 곳에서 추출된 특징과 매칭될 위험 있음.
- locality : 작은 영역을 중심으로 특징 벡터를 추출해야 물체에 occlusion이 발생해도 매칭이 안정적으로 동작한다.
- 적당한 양 : 특징점이 많으면 더 정확한 추적이 가능하나, 계산 시간이 과다해짐
- 계산 효율 : 실시간으로 처리해야 하는 경우 계산 시간에 중요한 응용이 필수적이다.

위의 6개 조건은 **특징점 검출, 기술자 추출**을 구현하는 것에 있어서 만족해야 한다.

해당 조건들은 때로 trade-off 관계에 있어서, 적절히 조절해야 한다.

- 반복성을 높이면 실시간 처리가 느려짐
- 넓은 영역에서 특정 벡터를 추출하면 분별력 높아지나, 지역성이 낮아짐 (occlusion에 대처 어려움)

---

## 이동과 회전 불변한 지역 특징(5.2)

매칭의 단계 : 검출 , 기술 , 매칭 중 detection

### 모라벡 알고리즘

- SSD(Sum Of Squared Difference)

$$
S(v,u) = \sum_{y}\sum_{x}(f(y+v, x+u) - f(y,x))^2\quad\quad\quad(1.1)
$$

- original img(y,x)에서, 위의 식을 통해 S-map(v,u)을 추출한다.

![Untitled](SIFT%204e2724dd87e84ea58a1c4bffe46e9b32/Untitled.png)

**S-map의 픽셀 값에 따른 특징**

![Untitled](SIFT%204e2724dd87e84ea58a1c4bffe46e9b32/Untitled%201.png)

(a) : 코너

![Untitled](SIFT%204e2724dd87e84ea58a1c4bffe46e9b32/Untitled%202.png)

(b) : 경계

![Untitled](SIFT%204e2724dd87e84ea58a1c4bffe46e9b32/Untitled%203.png)

(c) : 주변에 화소의 변화 x

**특징 가능성 값 C**

$$
C = min(S(0,1),S(0,-1),S(1,0),S(-1,0))
$$

- S맵을 가지고 지역 특징으로 좋은 정도를 측정한다.
- C의 값이 클 수록 좋은 점수를 갖는다.(모든 방향으로 높은 값을 가지기 때문에)

모라벡 알고리즘의 한계점

- 한 화소에 대해 네 방향만 보고 특징점의 여부를 계산
    - 3x3 크기의 마스크를 쓰기 때문
- 잡음에 대한 대처 방안 없음
    - window 함수가 1과 0으로만 이루어져 있어서 noise에 취약
- **S(v,u)**의 최솟값만 고려

---

## 해리스 특징점

논문 : Harris1988

Harris Detector

- Edge 검출 가능
- Edge Tracking에 초점을 두었음(Corner detection은 부산물)

$$
S(v,u) = \sum_{y}\sum_{x}G(y,x)(f(y+v,x+u)-f(y,x))^2\quad\quad\quad   (1.1)
$$

여기서 추가된 **G(y,x)**는 잡음에 대처하기 위해 추가된 가우시안이다.

Taylor Expension

$$
f(y+v,x+u)\approx f(y,x) + vd_y(y,x) + ud_x(y,x)\quad\quad\quad (1.2)
$$

(1.2)를 (1.1)에 대입

$$
S(v,u) \approx \sum_{y}\sum_{x}G(y,x)(vd_y(y,x)+ud_x(y,x))^2\quad\quad\quad
$$

$$
S(v,u) \approx \sum_y\sum_xG(y,x)(v^2d^2_y+2vud_yd_x + u^2d^2_x) 
$$

$$
S(v,u) \approx \sum_y\sum_xG(y,x)(v\space \space u)
   \begin{bmatrix} 
   d_y^2 & d_yd_x  \\
   d_yd_x & d_x^2  \\
   \end{bmatrix} 
   \begin{bmatrix} 
   v  \\
   u   \\
   
   \end{bmatrix} 
 \newline = (v\space \space u)\sum_y\sum_xG(y,x)
   \begin{bmatrix} 
   d_y^2 & d_yd_x  \\
   d_yd_x & d_x^2  \\
   \end{bmatrix} 
   \begin{bmatrix} 
   v  \\
   u   \\
   
   \end{bmatrix} 
 \newline = (v \space \space u)
   \begin{bmatrix} 
   \sum_y\sum_xG(y,x)d_y^2 & \sum_y\sum_xG(y,x)d_yd_x  \\
   \sum_y\sum_xG(y,x)d_yd_x & \sum_y\sum_xG(y,x)d_x^2 \\ 
   \end{bmatrix}    \begin{bmatrix} 
   v  \\
   u   \\
   
   \end{bmatrix} 

$$

$$
S(v,u) = (v \space \space u)
   \begin{bmatrix} 
   G\ast d_y^2 & G\ast d_yd_x  \\
   G\ast d_yd_x & G\ast d_x^2  \\

   \end{bmatrix}\begin{bmatrix} 
   v  \\
   u   \\
   
   \end{bmatrix}  
 \space= uAu^T \quad \quad\quad (1.3)
$$

**Second moment matrix** (위의 식에서 행렬 A)

$$
A = 
   \begin{bmatrix} 
   G\ast d_y^2 & G\ast d_yd_x  \\
   G\ast d_yd_x & G\ast d_x^2  \\

   \end{bmatrix} 
 \quad\quad\quad(1.4)
$$

- S-map을 구할 때 정수 뿐 아니라 실수도 가능
- 화소 주위의 영상 구조를 표현
    - A만 분석하면 지역 특징 여부 판단 가능
    - (v u)를 변화시키면서 맵을 생성하는 계산 과정 불필요
- A의 고윳값을 통해 지역 특징으로 좋은 정도 측정 가능

**특징 가능성 값 C**

$$
C = \lambda_1\lambda_2-k(\lambda_1+\lambda_2)^2\quad\quad (1.5)
$$

k는 0.04로 설정하는 것이 적절

고윳값의 크기에 따른 특징

1. λ가 모두 0

특징으로서의 가치 x

1. λ가 하나만 큼

한 방향으로만 변화 있음

1. λ가 모두 큼

지역 특징으로써 매우적합

$$
Let \quad A = 
   \begin{bmatrix} 
   p & r\\
   r & q\\

   \end{bmatrix} 
\space 
$$

$$
\quad \lambda_1 +\lambda_2 = p + q, \quad \lambda_1\lambda_2 =pq-r^2
$$

$$
C = (pq-r^2)-k(p+q)^2 \quad\quad\quad (1.6)
$$

(1.5)의 식은 λ값을 구해야 하는 계산이 필요하기 때문에 (1.6)의 식을 이용하여 코드를 작성한다.

특징 가능성 값 C :

![Untitled](SIFT%204e2724dd87e84ea58a1c4bffe46e9b32/Untitled%204.png)

![Untitled](SIFT%204e2724dd87e84ea58a1c4bffe46e9b32/Untitled%205.png)

- C값은 클 수록 높은 점수를 얻는다.

localization

- 특징 가능성 맵 C에서 특징일 가능성이 가장 높은 곳을 선택해야 함
- extreme point를 찾기 위해 비최대 억제 사용
    - 주변 8개의 이웃보다 큰 값을 갖는 화소를 특징점으로 선택

### 해리스 특징점 검출 알고리즘

- 식 두 개를 활용하여 코드를 구현한다.

![Untitled](SIFT%204e2724dd87e84ea58a1c4bffe46e9b32/Untitled%206.png)

![Untitled](SIFT%204e2724dd87e84ea58a1c4bffe46e9b32/Untitled%207.png)

![Untitled](SIFT%204e2724dd87e84ea58a1c4bffe46e9b32/Untitled%208.png)

Algorithm

1. 특징 가능성 맵 찾기
    - 10x10 matrix 생성
    - ux, uy vector 생성 [-1,0,1], [-1,0,1].T
    - 3x3(σ = 1) Gaussian 생성 (g)
    - img와 uy 컨볼루션 (dy)
    - img와 uv 컨볼루션 (dx)
    - dyy(dy*dy)와 g 컨볼루션 (gdyy)
    - C = (gdyy*gdxx - gdyx*gdyx) - 0.04*(gdyy+gdxx)*(gdyy+gdxx)

1. 비최대 억제를 사용하여 특징점 찾기
    - 특징점은 9로 표시

parameter 

- σ(gaussian) = 1

- 코드
    
    ```cpp
    #include <iostream>
    #include <opencv2/opencv.hpp>
    #define PI 3.141592653589793238462643383279502884L
    
    using namespace std;
    using namespace cv;
    
    Mat float_draw(Mat&);
    Mat Gaussian_Kernel(int, double);
    Mat conv_img(Mat&, Mat&);
    Mat non_maximum_suppression(Mat&, Mat&);
    int main() {
        //step 1 : 10x10 img 생성
        Mat img = (Mat_<double>(10, 10) <<
            0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
            0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
            0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
            0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
            0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
            0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
            0, 0, 0, 1, 1, 1, 1, 1, 0, 0,
            0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
            0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
            0, 0, 0, 0, 0, 0, 0, 0, 0, 0);
        
        //step2
        Mat ux = (Mat_<double>(1, 3) << -1, 0, 1);
        Mat uy = ux.t();
        Mat dy = conv_img(img, uy);
        Mat dx = conv_img(img, ux);
        Mat dyy = dy.mul(dy);
        Mat dyx = dy.mul(dx);
        Mat dxx = dx.mul(dx);
        
        //step3 Gaussian filtering
        Mat g = Gaussian_Kernel(3, 1);
        Mat gdyy = conv_img(dyy, g);
        Mat gdxx = conv_img(dxx, g);
        Mat gdyx = conv_img(dyx, g);
        print(g);
    
        //step4 calculate C
        Mat C = (gdyy.mul(gdxx) - gdyx.mul(gdyx)) - 0.04 * ((gdxx + gdyy)).mul((gdxx + gdyy));
    
    		//step5 non maximum suppression
        Mat non_max = non_maximum_suppression(img, C);
    
        //draw img
        Mat img_draw = float_draw(img);
        Mat img_conv_draw_dx = float_draw(dx);
        Mat img_conv_draw_dy = float_draw(dy);
        Mat img_conv_draw_dyy = float_draw(dyy);
        Mat img_conv_draw_dyx = float_draw(dyx);
        Mat img_conv_draw_dxx = float_draw(dxx);
        Mat img_conv_draw_gdyy = float_draw(gdyy);
        Mat img_conv_draw_gdyx = float_draw(gdyx);
        Mat img_conv_draw_gdxx = float_draw(gdxx);
        Mat img_conv_draw_C = float_draw(C);
        Mat img_non_max_draw = float_draw(non_max);
    
        namedWindow("Original Image", WINDOW_AUTOSIZE);
        imshow("Original Image", img);
    
        namedWindow("Drawn Image", WINDOW_AUTOSIZE);
        imshow("Drawn Image", img_draw);
        imshow("dx", img_conv_draw_dx);
        imshow("dy", img_conv_draw_dy);
        imshow("dyy", img_conv_draw_dyy);
        imshow("dyx", img_conv_draw_dyx);
        imshow("dxx", img_conv_draw_dxx);
        imshow("gdyy", img_conv_draw_gdyy);
        imshow("gdyx", img_conv_draw_gdyx);
        imshow("gdxx", img_conv_draw_gdxx);
        imshow("C", img_conv_draw_C);
        imshow("non_max", img_non_max_draw);
        waitKey();
        return 0;
    }
    
    Mat non_maximum_suppression(Mat& img_origin, Mat& img_C) {
        Mat img_non_max = img_origin.clone();
        for (int y = 0; y < img_origin.rows; y++) {
            for (int x = 0; x < img_origin.cols; x++) {
                int count = 0;
                if (img_C.at<double>(y, x) > 0.1) {
                    for(int m = -1; m < 2; m++){
                        for (int n = -1; n < 2; n++) {
                            if (img_C.at<double>(y, x) > img_C.at<double>(y + m, x + n)) count++;
                        }
                    }
                }
                if (count == 8) img_non_max.at<double>(y, x) = 9;
            }
        }
        return img_non_max;
    }
    
    Mat conv_img(Mat& img, Mat& kernel) {
        int kernel_x = (kernel.cols - 1) / 2;
        int kernel_y = (kernel.rows - 1) / 2;
        Mat img_conv = img.clone();
        Mat img_padded; copyMakeBorder(img, img_padded, kernel_y, kernel_y, kernel_x, kernel_x, BORDER_CONSTANT, 0);
        for (int i = kernel_y; i < img_padded.rows - kernel_y; i++) {
            for (int j = kernel_x; j < img_padded.cols - kernel_x; j++) {
                double value = 0;
                for (int m = 0; m < kernel.rows; m++) {
                    for (int n = 0; n < kernel.cols; n++) {
                        value += img_padded.at<double>(i - kernel_y + m, j - kernel_x + n) * (kernel.at<double>(m, n));
                    }
                }
    
                img_conv.at<double>(i - kernel_y, j - kernel_x) = value;
            }
        }
        return img_conv;
    }
    
    Mat float_draw(Mat& img) {
        double scale1 = 512.0 / img.rows;   // 행의 scale
        double scale2 = 512.0 / img.cols;   // 열의 scale
    
        Mat img_draw(img.rows * scale1, img.cols * scale2, CV_32FC1, Scalar(0.0f));
    
        for (int i = 0; i < img.rows; i++)
            for (int j = 0; j < img.cols; j++) {
                string text = to_string(img.at<double>(i, j));
                int text_width, text_height;
    
                Size text_size = getTextSize(text, FONT_HERSHEY_SIMPLEX, 0.3, 1, nullptr);
                text_width = text_size.width;
                text_height = text_size.height;
                int x = j * scale2 + (scale2 - text_width) / 2;
                int y = i * scale1 + (scale1 + text_height) / 2;
    
                putText(img_draw, text, Point(x, y), FONT_HERSHEY_SIMPLEX, 0.3, Scalar(255), 1, LINE_AA);
            }
        return img_draw;
    }
    
    Mat Gaussian_Kernel(int size, double sigma) {
        Mat kernel(size, size, CV_64F);
    
        int kernelCenter = (size - 1) / 2;
        double sum = 0.0;
    
        for (int i = 0; i < size; i++) {
            for (int j = 0; j < size; j++) {
                int x = kernelCenter - i;
                int y = kernelCenter - j;
                kernel.at<double>(i, j) = exp(-(static_cast<double>(x * x) + static_cast<double>(y * y)) / (2.0 * sigma * sigma)) / sqrt(2.0 * PI * sigma * sigma);
                sum += kernel.at<double>(i, j);
            }
        }
        kernel /= sum;
        return kernel;
    }
    ```
    
- 결과
    
    
    ![Untitled](SIFT%204e2724dd87e84ea58a1c4bffe46e9b32/Untitled%209.png)
    
    - origin image
    - C
    - non_maximum_suppression
    
    ![Untitled](SIFT%204e2724dd87e84ea58a1c4bffe46e9b32/Untitled%2010.png)
    
    ![Untitled](SIFT%204e2724dd87e84ea58a1c4bffe46e9b32/Untitled%2011.png)
    
    결과값을 보면 세 특징점이 모두 모퉁이(corner)에 존재하지만, 실제 영상에 적용하면 모퉁이(corner) 뿐만 아니라 블롭(blob)도 많이 검출한다.
    
    따라서 이렇게 검출된 값은 feature point 또는 interest point 라고 한다.
    

---

## 스케일 불변한 지역 특징(5.3)

스케일 공간 이론 :

- 스케일을 모르는 상황에 대비하기 위해 위의 Algorithm을 사용
- 위의 알고리즘을 구현하는 다양한 변형 중 SIFT가 가장 성공한 사례

Algorithm

![Untitled](SIFT%204e2724dd87e84ea58a1c4bffe46e9b32/Untitled%2012.png)

- 입력 : 명암 영상 f
- 출력 : 스케일에 불변한 특징점 집합

Algorithm_1 : 다중 스케일 영상 구성 방법

1. 가우시안 스무딩 
    - 표준편차 σ를 점점 키우면서 가우시안 필터로 입력 영상 스무딩
    - 거리가 멀어지면 물체가 흐려지는 것을 묘사
    - σ를 연속된 값으로 조절 가능
    - 가우시안 스무딩으로 만든 영상을 t축에 배치
        - (y,x,t) 공간을 스케일 공간이라 함.
2. 피라미드 방법
    - 거리가 멀어지면 물체의 크기가 작아지는 것을 묘사
    - 연속적인 스케일로 줄일 수 없는 한계가 있음.

Algoritm_2 : 스케일 공간의 미분

Laplacian 주로 사용

- scale space에서 극점 찾기에 유리
- σ가 클수록 d_xx와 d_yy가 작아지는 문제가 있어서 식 (1.7)보다는 (1.8)을 많이 씀

$$
\nabla^2f = d_{yy}+d_{xx} \quad\quad (1.7)
$$

$$
\nabla^2_{normal}f=\sigma^2|d_{yy} + d_{xx}| \quad\quad (1.8)
$$

(1.7) : Laplacian                                                                            (1.8) : normalized Laplacian

Algoritm_3 : 극점 검출

- 3차원에서의 비최대 억제를 적용
    - 스케일 공간은 3차원이기 때문

---

# Lowe 2004

## 1. Scale-space extrema detection

### 1.1 Detection of scale-space extrema

Keypoint detection의 첫 번째 과정은 locations와 scales를 찾는 것 이다.

$$
L(x,y,\sigma) = G(x,y,\sigma)*I(x,y)
$$

input image : I(x, y)

a variable-scale Gaussian: G(x, y, σ)

the scale space of an image : L(x, y, σ)

$$
D(x,y,\sigma) = (G(x,y,k\sigma)-G(x,y,\sigma))*I(x,y) \\ =L(x,y,k\sigma) - L(x,y,\sigma)
$$

the difference-of-Gaussian function : D(x, y, σ)

constant multiplicative factor : k

$$
\frac{\partial G}{\partial\sigma}=\sigma\nabla^2G \quad \quad (2.1)
$$

$$
열\space확산\space방정식\space\frac{\partial T}{\partial t} =D\nabla^2T \quad \quad (2.2)
$$

(2.2)의 열 확산 방정식 원리와 (2.1)의 식을 이용하여 아래 식 유도

$$
\sigma\nabla^2G=\frac{\partial G}{\partial\sigma} \approx \frac{G(x,y,k\sigma)-G(x,y,\sigma)}{k\sigma-\sigma}
$$

$$
G(x,y,k\sigma)-G(x,y,\sigma) \approx (k-1)\sigma^2\nabla^2G
$$

**Difference-of-Gaussian**

$$
(k-1)\sigma^2\nabla^2G
$$

**Scale-normalized Laplacian of Gaussian**

$$
\sigma^2\nabla^2G
$$

계산이 복잡한 **LoG를 DoG로 대체** 가능하다

- 즉, 단순한 이미지 빼기 연산으로 계산 가능
- 여기서 (k-1)은 extrema loaction에 영향을 주지 않는다.

$$
k=2^{\frac{1}{s}}
$$

must poduce **s + 3 images** in the stack of blurred images for each octave

![Untitled](SIFT%204e2724dd87e84ea58a1c4bffe46e9b32/Untitled%2013.png)

### 1.2 **Local extrema detection**

D(x,y,σ)의 extrema 찾기

- maxima and minima → 이웃의 26개의 img 비교 하여 찾기

![Untitled](SIFT%204e2724dd87e84ea58a1c4bffe46e9b32/Untitled%2014.png)

### 1.3 **Frequency of sampling in scale**

- 샘플링 빈도 찾기( 효율성 vs 안정성 )
    - 샘플링을 많이 하면 안정성이 올라가나 효율성이 떨어짐
    - 샘플링을 적게 하면 효율성이 올라가나 안정성이 떨어짐

오른쪽의 두 표는 야외 장면, 인간의 얼굴, 항공 사진 및 산업 등 32개의 다양한 범위의 이미지에 또한, 회전, 확대/축소, 뒤틀림, 밝기 및 대비, 노이즈 등 다양한 변환들이 적용된 이미지들을 넣고 실험을 진행한 것이다.

무작위 각도로 회전하고, 원본 크기의 0.2배에서 0.9배 사이로 임이의 양만큼 크기 조절 후 resampling되었고, 이미지에 1% 미만의 노이즈 추가함

The top line : 변환된 이미지에서 일치하는 위치와 스케일에서 감지된 키포인트의 백분율

The lower line : 40000개의 키포인트 데이터베이스에 대해 최근접 이웃 매칭 절차를 사용하여 정확하게 매치된 키포인트의 수

결론

he reason is that this results in many more local extrema being detected,
but these extrema are on average less stable and therefore are less likely to be detected in the transformed image.

![Untitled](SIFT%204e2724dd87e84ea58a1c4bffe46e9b32/Untitled%2015.png)

- octave가 3일때 재현율이 가장 높다.

Repeatability(재현율) :  서로 다른 이미지나 동일한 이미지의 다양한 조건에서 SIFT keypoint가 얼마나 일관되게 검출되는지를 평가

![Untitled](SIFT%204e2724dd87e84ea58a1c4bffe46e9b32/Untitled%2016.png)

- octave의 개수 별로 keypoint를 나타냄

### 1.4 **Frequency of sampling in the spatial domain**

trade-off

- **sampling frequency** and **rate of detection**
- σ를 높이면 repeatability 올라감
- cost도 같이 상승하여 rate of detection이 안좋아짐
- 적절한 σ = 1.6

![Untitled](SIFT%204e2724dd87e84ea58a1c4bffe46e9b32/Untitled%2017.png)

원본 이미지 σ = 0.5를 가지고 있다고 가정

입력 이미지의 크기를 2배로 확장하면  안정적인 키포인트 수를 거의 4배 증가.

- (σ = 1이 되어 추가적인 평활화는 필요 x)
- 더 큰 확장계수는 유의미한 추가 향상 x

## 2. Keypoint localization

### 2.1 Accurate Keypoint localization

키포인트 후보가 이웃 픽셀과 비교를 통해 찾아진 후에 **해당 위치, 스케일 및 주요 곡률 비율에 대한 상세한 적합(fit)**을 수행해야함

- low contras와 poorly localized along an edge 제거에 사용
- Scale-space 공간 함수 D(x,y,σ)의 Taylor expansion 사용

$$
D(x) = D +\frac{\partial D^T}{\partial x}+\frac{1}{2}x^T\frac{\partial^2D}{\partial x^2}x\space,\quad x = (x,y,\sigma)^2
$$

$$
location\space of\space the \space extremum\space\hat{x}=-\frac{\partial^2D^{-1}}{\partial x^2}\frac{\partial D}{\partial x}
$$

$$
D(\hat{x})=D+\frac{1}{2}\frac{\partial D^T}{\partial x}\hat{x} \quad\quad (2.3)
$$

$$
Discard \space if \space |D(\hat{x})| \leq0.03 \quad\quad (2.4)\\\quad img\space pixel \space values\space in \space the \space range[0,1]
$$

The function value at the extremum (2.3)은 low contrast에서의 불안정한 극점 제거에 용이

- Low contrast : 특정 지역이나 물체가 주변과 비교했을 때 그 차이가 작아서 시각적인 구별 x

![Untitled](SIFT%204e2724dd87e84ea58a1c4bffe46e9b32/Untitled%2018.png)

(a) The 233x189 pixel original image.

![Untitled](SIFT%204e2724dd87e84ea58a1c4bffe46e9b32/Untitled%2019.png)

(b) The initial 832 keypoints **locations at maxima and minima of the difference-of-Gaussian function**. 

Keypoints are displayed as vectors indicating scale, orientation, and location.

![Untitled](SIFT%204e2724dd87e84ea58a1c4bffe46e9b32/Untitled%2020.png)

(c) After applying a **threshold on minimum contrast**, 729 keypoints remain. **-  (2.4) 적용** 

![Untitled](SIFT%204e2724dd87e84ea58a1c4bffe46e9b32/Untitled%2021.png)

The final 536 keypoints that remain following **an additional threshold on ratio of principal curvatures - (2.5) (2.6) 적용**

### 2.2 **Eliminating edge responses**

stability를 위해서 low contrast 뿐만 아니라, principal curvature를 통해 나머지 노이즈도 제거해야함

**Hessian matrix H**

$$
\textbf{H}=
   \begin{bmatrix} 
   D_{xx} & D_{xy}\\
   D_{xy} & D_{yy}\\

   \end{bmatrix} 

$$

**H**의 고유값들은 D의 주된 곡률(principal curvature)와 비례, 고윳값 계산할 필요 없음

α를 가장 큰 크기의 고윳값으로 가정

$$
Tr(\textbf{H})=D_{xx}+D_{yy}=\alpha+\beta
$$

$$
Det(\textbf{H})=D_{xx}D_{yy}-(D_{xy})^2=\alpha\beta \\ Det(\textbf{H})>0 \quad \quad (2.5)
$$

H는 정방행렬이므로 위의 식이 성립

$$
\frac{Tr(\textbf{H})^2}{Det(\textbf{H})}=\frac{(\alpha+\beta)^2}{\alpha\beta}=\frac{(r\beta+\beta)^2}{r\beta^2}=\frac{(r+1)^2}{r}
$$

$$
\frac{Tr(\textbf{H})^2}{Det(\textbf{H})}<\frac{(r+1)^2}{r} \quad \quad (2.6)
$$

여기서 

- 모든 고윳값이 **양수**인 경우 :
    - 위로 볼록 (함수는 극솟값을 가짐)
    - α >> β → **Edge**
- 모든 고윳값이 **음수**인 경우 :
    - 아래로 볼록 (함수는 극댓값을 가짐)
    - α << β → **Edge**
- 위 두 경우에 대해 α 와 β가 비슷하면 **corner**
- 고윳값의 부호가 다를 경우 :
    - 극값이 아니라는 것을 의미

즉, 아래 두 가지 조건을 시행한다.

1. Det(H)<0 인 경우 제거
    - 주된 곡률이 서로 다른 부호를 가지므로 해당 포인트는 극값이 아니라고 판단
2. r = 10의 값(논문에서 사용)을 사용하여 주된 곡률의 비율이 10보다 큰 키 포인트를 제거 
    - r의 값이 클 수록 주된 곡률 간의 크기 차이가 크다는 것을 나타내고, 이는 안정성이나 신뢰성이 떨어질 가능성이 높다.

## 3. Orientation assignment

각 키포인트에 대해 로컬 이미지 속성을 기반으로 일관된 방향 할당함으로써 키포인트 기술자는 이 방향을 기준으로 상대적으로 표현되어 이미지 회전에 대한 불변성 달성 가능

각각의 **img sample L(x,y)**에 대해 **gradient magnitude m(x,y)와 orientation θ(x,y)** 계산

$$
m(x,y)=\sqrt{((L(x+1,y)-L(x-1,y))^2+((L(x,y+1)-L(x,y-1))^2}
$$

$$
\theta(x,y)=tan^{-1}((L(x,y+1)-L(x,y-1))/(L(x+1,y)-L(x-1,y)))
$$

1. 검출된θ의 방향을 360도 기준으로 10도 방향으로 총 36개의 bin을 가지는 orientation histogram 생성
2. θ별로 해당하는 magnitude x (키포인트의 스케일 x 1.5) 을 통해 얻은 값을 orientation histogram에 저장. 
3. 가장 높은 peak의 방향으로 keypoint 생성 + 가장 높은 peak의 80% 이상인 다른 로컬 피크가 있으면 해당 방향으로도 keypoint 생성
    - 약 15%정도의 포인트가 생성됨(안정성에 중요한 기여)
4. 가장 높은 peak 값 3개의 좌표를 이용하여 parabola를 적합시킴

해당 표는 다양한 image noise에 대한 location, scale, orientation assignment 안정성을 보여주고 있다.

중간의 줄을 보면 방향이 추가된 것을 알 수 있는데, 여기서 위의 과정을 적용한 뒤이기 때문에 방향 할당이 15도 이내가 된다.

방향 할당이 15도 이내여도 95%의 정확도로 안정적으로 유지됨을 나타내고 있다.(img noise가 10% 추가된 경우까지도)

맨 아래 그래프는 40000개의 keypoint database에 대한 키포인트 기술자를 올바르게 일치시키는 최종 정확도를 보여준다.

즉, 해당 그래프는 노이즈에 강인하고, 오류의 주된 원인은 초기 위치 및 스케일 감지에 있다는 것을 알려준다.

![Untitled](SIFT%204e2724dd87e84ea58a1c4bffe46e9b32/Untitled%2022.png)

## 4. Keypoint descriptor

### 4. The local image descriptor

각 키포인트에 대해 이미지 위치, 스케일 및 방향을 할당했으니, 이제 해당 지역 이미지 영역에 대한 기술자(descriptor)를 계산

- 조명 변경이나 3D 시각점 등의 다양한 변형에 가능하게 한 불변성 제공

### 4.1 Descriptor representation

키포인트 위치 주변에서 이미지 그라디언트 크기와 방향 샘플링

키 포인트의 스케일을 사용하여 이미지에 대한 가우시안 블러 수준 선택

**to achieve orientation invariance**

- descriptor의 좌표와 gradient의 방향은 keypoint의 방향을 기준으로 회전.
- 회전된 좌표와 방향 정보를 사용하여 16x16 window를 4x4 subregion으로 나눔
- 각 subregion 내에서 gradient magnitudes의 합을 사용하여 keypoint descriptor 생성
- 각 샘플 포인트 크기에 가중치 할당을 위해 descriptor window의 너비의 절반의 크기인 σ를 갖는 가우시안 가중치 함수 사용
    - Gaussian window의 목적
        - window의 작은 위치 변경에 대한 descriptor의 급격한 위치 변화를 피하는 것
        - descriptor의 centor로부터 멀리 떨어진 gradient들에 대해  중점을 덜 두기 위함. (misregistration errors 방지)
        

![Untitled](SIFT%204e2724dd87e84ea58a1c4bffe46e9b32/Untitled%2023.png)

- keypoint descriptor는 총 8개의 방향(45도)로 나눔
- descriptor가 갑자기 변하는 경우
    - sample이 한 히스토그램에서 다른 히스토그램으로 부드럽게 이동
    - 하나의 방향에서 다른 방향으로 이동
    - boundary affects 방지책
        - trilinear interpolation 사용하여 각 기울기 샘플의 값을 인접한 히스토그램 항목으로 분산 (x,y,d를 사용)
        - 각 항목(bin)은 각 차원에 대해 (1-d)의 가중을 받는다. (d는 bin의 central value로부터 각 unit까지의 거리)
- 논문에서는 Keypoint descriptor가 총 4x4개 만큼 있다.
- 즉 각 Keypoint마다 (4x4 array) x (8 orientation) = 128 element feature vector를 사용한다.
- 마지막으로 feature vector를 illumination 변화의 영향을 줄이기 위해 수정
    1. 선형 조명 변화의 경우 벡터를 단위 길이로 정규화
        - 각 픽셀 값이 상수로 곱해지는 이미지 대비 변경은 그라디언트를 동일 상수로 곱하기 때문에 위의 정규화에 의해 취소된다.
            - 그라디언트는 픽셀간 차이에서 계산되기 때문에 그렇다.
            - 따라서 이런 조명에 대한 아핀 변화에 대해 기술자는 불변
    2. 비선형 조명 변화의 경우 단위 벡터의 각 값을 0.2보다 크지 않도록 임계 처리 후 단위길이로 정규화하여 큰 경사크기의 영향 감소 
        - 큰 경사 크기가 특징을 덮어버리는 것을 방지하여 조명변화에 민감하지 않도록 만듦

### 4.2 Descriptor testing

two parameters

- the number of orientations r
- width n, of the nxn array orientation histograms

이 두 파라미터를 이용해 복잡성 변화시킬 수 있음

Trade-off

- 복잡성과 민감성
    - 복잡성이 증가하면 더 나은 구별력을 가짐
    - 복잡성이 증가하면 형태의 왜곡과 가려짐에 더 민감해짐

해당 표는 평면 표면이 뷰어로부터 50도 기울어지고 

4%의 노이즈가 추가된 시각변환임(디스크립터의 성능이 가장 중요한 경우)

40,000개의 키포인트 데이터베이스에서 가장 가까운 이웃 중에서 

올바른 일치를 찾은 키포인트의 비율을 보여줌

해당 논문은 4x4 descriptor를 사용함.

![Untitled](SIFT%204e2724dd87e84ea58a1c4bffe46e9b32/Untitled%2024.png)

### 4.3 Sensitivity to affine change

해당 그래프는 뷰어로부터의 angle이 증가함에 따라 repeatability의 변화를 표현

![Untitled](SIFT%204e2724dd87e84ea58a1c4bffe46e9b32/Untitled%2025.png)

### 4.4 Matching to affine change

이 논문의 대부분의 예제 40000개의 키포인트를 포함하는 32개의 이미지의 데이터베이스를 사용하여 생성되었음.

해당 그래프는 112개의 이미지의 데이터베이스를 사용하여 생성하였고, 30도의 affine 변환과 2% 의 노이즈를 추가하였다.

점선 : 데이터 베이스 크기의 로그 스케일에 따른 올바른 일치의 특정 비율

실선 : 변환된 이미지에서 올바른 위치와 방향에 있는 키포인트의 백분율

두 선 간의 갭이 작다는 것의 의미

- 초기 특징 지역화 및 방향 할당 문제보다는 특징의 독립성 문제때문이다.
    - 특징의 독립성 문제 :
        
         매칭이 특징 서술자의 유일성과 중복성에 의지하는 정도
        

![Untitled](SIFT%204e2724dd87e84ea58a1c4bffe46e9b32/Untitled%2026.png)

---

### SIFT의 특징점 검출 3단계

**1단계 : 다중 스케일 영상 구축** 

Gaussian Smoothing과 Pyramid 방법을 결합해 다중 스케일 영상을 구성한다.

1. octave 0 :
    - 원래 영상을 스무딩한 영상에서 출발(원본 영상 0.5 스무딩 되어있다고 가정)
    
    $$
    \sigma_{2} = k\sigma_1, \quad k = 2^{\frac{1}{3}}
    $$
    

$$
\sigma_1 =\sqrt{1.6^2-0.5^2} =1.5199,\\\space \sigma_2= 1.9149, \space \sigma_3=2.4126, \space \sigma_4=3.0397, \space \sigma_5=3.8298,\space \sigma_6=4.8253
$$

$$
여기서, 원본 영상을 \sigma_{i+1}로\space스무딩한 영상은 \sigma_i로\space스무딩한\space영상에\space \\\sqrt{\sigma_{i+1}^2-\sigma_{i}^2} \space 값으로 스무딩 한 것과 같다.
$$

- i가 클수록 필터가 커서 오래걸리기 때문에 컨볼루션의 수학적 특성을 이용해 스무딩한다.
- *σ*가 두배가 될 때까지 한 옥타브로 묶음
    - *σ*가 두배라는건 거리가 두배라는 것과 동치?
    - 그럼 4번째 *σ*까지만 하면 되나?

1. octave 0→1(다운 샘플링)

**2단계 : 다중 스케일 영상에 미분 적용**

SIFT 사용 수식

![Untitled](SIFT%204e2724dd87e84ea58a1c4bffe46e9b32/Untitled%2027.png)

- DOG(Difference of Gaussian)
    - 정규 라플라시안과 매우 유사하다고 증명된 DOG를 사용
    - 계산 시간 감소
    - LoG(Laplacian of Gaussian)은 연산량이 많기 때문에 DOG 사용

![Untitled](SIFT%204e2724dd87e84ea58a1c4bffe46e9b32/Untitled%2028.png)

**3단계 : 극점 검출**

해리스 특징점 검출(2차원)과 방식은 동일하나, 차원이 하나 추가됨

1. 특징 가능성 맵 찾기(3차원)
2. 비최대 억제를 사용하여 특징점 찾기
    - SIFT에서 특징점을 keypoint라고 부름

26개의 이웃 화소보다 크거나 작으면 극점으로 인정하고 keypoint로 취한다.

- 즉, 극댓값과 극솟값을 찾는다

![Untitled](SIFT%204e2724dd87e84ea58a1c4bffe46e9b32/Untitled%2029.png)

![Untitled](SIFT%204e2724dd87e84ea58a1c4bffe46e9b32/Untitled%2030.png)

![Untitled](SIFT%204e2724dd87e84ea58a1c4bffe46e9b32/Untitled%2031.png)

검출된 **keypoint**는 **(y,x,o,i)**로 표현한다.

- o : octave,      i : DOG number
- 옥타브 o에 있는 i번째 DOG의 위치 (y,x)에서 검출되었다는 의미
- 스케일 정보를 나타내는 변수(o,i), 위치를 나타내는 변수(y,x)를
    
    테일러 확장으로 미세 조정하여 최종 확정
    
- 마지막으로 에지에서 검출된 특징점을 걸러내는 후처리 과정 적용

위의 과정들을 통해 SIFT의 특징점의 위치와 스케일 정보를 알아낼 수 있음.

---

### SIFT Descriptor

특정 벡터에 해당하는 **기술자**를 특징점 주위를 살펴서 추출하는 과정이다.

**Algorithm**

**스케일 불변성** : 

- (y,x,o,i)는 octav o에 있는 i번째 DOG의 위치 (y,x)에서 검출되었다는 의미
- 이 정보를 이용해 가장 가까운 가우시안 영상을 결정하고 거기서 기술자를 추출
- 이 과정을 통해 스케일 불변성 달성

**회전 불변성** :

- keypoint 주변 gradient 크기 및 방향  알아내기

![Untitled](SIFT%204e2724dd87e84ea58a1c4bffe46e9b32/Untitled%2032.png)

- 키포인트 주위 영역에서 추출한 그래디언트 분포를 이용하여 기준 방향 결정
- 특징점 미세 조정 단계에서 x,y가 실수가 되었으므로 양선형 보간법을 이용해 작은 영역 샘플링
- 이 영역에 대해 소벨 연산자로 그레디언트의 강도와 방향 계산
- 그레디언트 방향을 10도 간격으로 양자화해서 36개 칸을 가진 히스토그램 구하기
- 히스토그램에서 최댓값을 가진 칸을 찾아서 dominant orientation으로 정함
- 만약 최대값의 0.8배 이상인 다른 칸이 있으면 그것도 dominant orientation
- 따라서 하나의 키포인트에서 방향이 다른 여러 특징점 발생 가능
- keypoint ⇒ (y,x,σ,*θ)*
    - σ : o와 i를 변환, *θ : dominant orientation*
- *θ*를 기준으로 window를 씌우고 보간 방법으로 샘플링하여 16x16의 작은 영역을 얻는다. 그 뒤, 4x4크기의 블록 16개로 나눈다. 각 블록은 자신이 속한 16개 화소의 그래디언트 방향을 8단계로 양자화하고 히스토그램을 구한다. 이때 가우시안을 가중치로 사용한다.
- 블록이 16개, 블록마다 8차원 히스토그램이 만들어지므로 16 x 8 = 128차원의 기술자 x를 얻는다.

**조명 불변성** :

- 영상이 밝아지면 특징값이 커지고, 어두워지면 작아진다.
- 따라서 기술자 **x**를 벡터의 크기로 나눠 단위 벡터로 바꾼다.
- 여기서 단위 벡터에 0.2보다 큰 요소가 있으면 0.2로 바꾼다음 다시 단위벡터로 바꾼다 ?
- 이렇게 얻은 x : 최종 기술자.
- 이제 특징점은 (y,x,σ,*θ,***x***)*로 표기

- 코드 알고리즘
    1. Scale-space extrema detection
        1. 다중 스케일(y,x,t) 즉, sigma 바꿔가며 여러 옥타브 생성
            1. sigma는 4까지(이래야 2배 증가) 
            2. Octave는 4개(근데 첫 Octave는 원본 이미지의 2배?)
            3. blog에선 보통 sigma 1~5, octav 4개 하고있
        2. DoG를 이용해 DoG img(총 16장(원본 img 20장이므로)) 구해
            
            근데 구한 것이 (열 확산 방정식을 통해)
            
            ![Untitled](SIFT%204e2724dd87e84ea58a1c4bffe46e9b32/Untitled%2033.png)
            
            이거라서 k-1은 극대 극소 찾는데 영향 x이므로 Log가 Dog로 무리 없이 잘 대체된다.
            
            그럼 정규화가 되어있는건가? 블로그는 아니던데
            
            1. 아래 식은 scaled normalized LoG
                
                $$
                \sigma^2\nabla^2G
                $$
                
            2. i-1, i+1 Dog img(즉 img 3장 써서) 비교하여 극대, 극소구하기(이게 Keypoint)
                1. 그럼 Octave 하나당 Dog img가 4개였으니까 한 Octave당 2장씩 총 8장 극값 img 얻어냄
                2. 여기서 어떻게 좌표에 접근할거냐 바로 테일러 전개
                    
                    ![Untitled](SIFT%204e2724dd87e84ea58a1c4bffe46e9b32/Untitled%2034.png)
                    
                3. 그럼 극값 위치 결정
                
                ![Untitled](SIFT%204e2724dd87e84ea58a1c4bffe46e9b32/Untitled%2035.png)
                
            3. 그러면 (x,y,o,i)가 나오겠지
            4. 여기서 threshold 추가해서 나쁜 keypoint 제거
                1. 이 값의 절대값이 특정 값보다 작으면 그 위치 keypoint에서 제거
                
                ![Untitled](SIFT%204e2724dd87e84ea58a1c4bffe46e9b32/Untitled%2036.png)
                
                 b.  여기서 더 확실한 코너점만 Keypoint로 남겨주기
                
                이거 엣지 찾기 위해 해시안 행렬 사용 
                
                즉 엣지 위의 keypoint 제거
                
                ![Untitled](SIFT%204e2724dd87e84ea58a1c4bffe46e9b32/Untitled%2037.png)
                
            5. r 10으로 설정 
            6. 여기까지 하면 scale 불변 만족
    2. Keypoint localization
        1. Keypoint 주변을 잘라(16개의 window로)
            1. 근데 그 window = 4x4 , 즉 Keypoint 주변 256개 픽셀을 잘라
            2. 그다음 가우시안 가중함수 곱
                1. 여기선 가운대 부분에 가중치 크게 주려고
                2. 즉 요소별 곱을 하기 
            3. 그 안의 모든 픽셀과 방향 크기 구하기
            
            ![Untitled](SIFT%204e2724dd87e84ea58a1c4bffe46e9b32/Untitled%2038.png)
            
            1. 그다음 잘린 그 픽셀을 저장하여 방향과 크기 행렬 생성해
                1. 여기서 방향은 36등분하여 저장(10도 단위로)
                2. 즉 bin은 (0~9도부터 시작해서 350~359까지) 총 36를 갖는 히스토그램 생성
                3. 가장 높은 bin을 찾고 여기서 80% 이상의 높이를 갖는 bin 있으면 그것도 방향으로 만들기
    3. Orientation assignment
    4. Keypoint descriptor